{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AjIjR-55snS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers datasets accelerate seqeval -q\n",
        "!pip install torch # Ensure torch is installed if not already\n",
        "!pip install seqeval -q\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset, Features, Value, ClassLabel\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from seqeval.metrics import classification_report\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW7DrBPxXFNE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset, Features, Value, Sequence, ClassLabel\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline\n",
        "from seqeval.metrics import classification_report\n",
        "import numpy as np\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "conll_file_path = '/content/drive/MyDrive/labeled_amharic_ner_data.conll'\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(conll_file_path):\n",
        "    print(f\"Error: CoNLL file not found at {conll_file_path}\")\n",
        "    print(\"Please ensure 'labeled_amharic_ner_data.conll' is in your Google Drive's root and the path is correct.\")\n",
        "else:\n",
        "    print(f\"CoNLL file found at {conll_file_path}\")\n",
        "\n",
        "# Definelabels\n",
        "label_list = ['O', 'B-Product', 'I-Product', 'B-LOC', 'I-LOC', 'B-PRICE', 'I-PRICE']\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "# Function to read CoNLL file\n",
        "def read_conll_file(file_path):\n",
        "    tokens = []\n",
        "    ner_tags = []\n",
        "    current_tokens = []\n",
        "    current_tags = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) == 2:\n",
        "                    current_tokens.append(parts[0])\n",
        "                    current_tags.append(parts[1].strip())\n",
        "            else:\n",
        "                if current_tokens:\n",
        "                    tokens.append(current_tokens)\n",
        "                    ner_tags.append(current_tags)\n",
        "                current_tokens = []\n",
        "                current_tags = []\n",
        "        if current_tokens:\n",
        "            tokens.append(current_tokens)\n",
        "            ner_tags.append(current_tags)\n",
        "    return {'tokens': tokens, 'ner_tags': ner_tags}\n",
        "\n",
        "# Load custom dataset\n",
        "raw_data_for_hf = read_conll_file(conll_file_path)\n",
        "\n",
        "# Define features for the dataset\n",
        "features_for_dataset = Features({\n",
        "    'tokens': Sequence(Value('string')),\n",
        "    'ner_tags': Sequence(ClassLabel(names=label_list))\n",
        "})\n",
        "\n",
        "# Create Hugging Face Dataset\n",
        "full_dataset = Dataset.from_dict(raw_data_for_hf, features=features_for_dataset)\n",
        "\n",
        "print(f\"Dataset loaded. Number of examples: {len(full_dataset)}\")\n",
        "print(\"First example:\", full_dataset[0])\n",
        "\n",
        "# Train-test split\n",
        "train_test_split = full_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
        "\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    report = classification_report(true_labels, true_predictions, output_dict=True, zero_division=0)\n",
        "    overall_metrics = report.get('micro avg', {})\n",
        "    f1_score = overall_metrics.get('f1', overall_metrics.get('f1-score', 0.0))\n",
        "    return {\n",
        "        \"precision\": overall_metrics.get('precision', 0.0),\n",
        "        \"recall\": overall_metrics.get('recall', 0.0),\n",
        "        \"f1\": f1_score,\n",
        "        \"accuracy\": overall_metrics.get('precision', 0.0)\n",
        "    }\n",
        "\n",
        "!rm -rf ./results_*\n",
        "!rm -rf ./logs_*\n",
        "!rm -rf ~/.cache/huggingface/\n",
        "!rm -rf /tmp/*\n",
        "\n",
        "model_checkpoints_to_compare = {\n",
        "    \"XLM-R_Amharic_NER\": \"mbeukman/xlm-roberta-base-finetuned-ner-amharic\",\n",
        "    \"mBERT\": \"bert-base-multilingual-cased\",\n",
        "    \"DistilBERT_Multi\": \"distilbert-base-multilingual-cased\",\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, checkpoint in model_checkpoints_to_compare.items():\n",
        "    print(f\"\\n--- Fine-tuning {model_name} ({checkpoint}) ---\")\n",
        "    !rm -rf /tmp/*\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        checkpoint,\n",
        "        num_labels=len(label_list),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    def tokenize_and_align_labels_for_current_model(examples):\n",
        "        tokenized_inputs = tokenizer(\n",
        "            examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "        )\n",
        "        labels = []\n",
        "        for i, label_ids_for_example in enumerate(examples[\"ner_tags\"]):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            previous_word_idx = None\n",
        "            current_label_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    current_label_ids.append(-100)\n",
        "                elif word_idx != previous_word_idx:\n",
        "                    current_label_ids.append(label_ids_for_example[word_idx])\n",
        "                else:\n",
        "                    original_word_label_id = label_ids_for_example[word_idx]\n",
        "                    original_word_label_name = id2label[original_word_label_id]\n",
        "                    if original_word_label_name.startswith(\"B-\"):\n",
        "                        new_label_name = f\"I-{original_word_label_name[2:]}\"\n",
        "                        current_label_ids.append(label2id.get(new_label_name, original_word_label_id))\n",
        "                    else:\n",
        "                        current_label_ids.append(original_word_label_id)\n",
        "                previous_word_idx = word_idx\n",
        "            labels.append(current_label_ids)\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs\n",
        "\n",
        "    current_tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels_for_current_model, batched=True)\n",
        "    current_tokenized_eval_dataset = eval_dataset.map(tokenize_and_align_labels_for_current_model, batched=True)\n",
        "\n",
        "    current_data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "    current_output_dir = f\"./results_{model_name}\"\n",
        "    current_logging_dir = f\"./logs_{model_name}\"\n",
        "    if os.path.exists(current_output_dir):\n",
        "        !rm -rf {current_output_dir}\n",
        "    if os.path.exists(current_logging_dir):\n",
        "        !rm -rf {current_logging_dir}\n",
        "    os.makedirs(current_output_dir, exist_ok=True)\n",
        "    os.makedirs(current_logging_dir, exist_ok=True)\n",
        "\n",
        "    training_args_current = TrainingArguments(\n",
        "        output_dir=current_output_dir,\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=7,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=current_logging_dir,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer_current = Trainer(\n",
        "        model=model,\n",
        "        args=training_args_current,\n",
        "        train_dataset=current_tokenized_train_dataset,\n",
        "        eval_dataset=current_tokenized_eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=current_data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    train_start_time = time.time()\n",
        "    trainer_current.train()\n",
        "    train_end_time = time.time()\n",
        "    training_time = train_end_time - train_start_time\n",
        "\n",
        "    eval_metrics = trainer_current.evaluate()\n",
        "\n",
        "    inference_start_time = time.time()\n",
        "    sample_text = \"አዲስ አበባ ላይ ቴሌቪዥን በ1000 ብር ይሸጣል\"\n",
        "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "    _ = ner_pipeline(sample_text)\n",
        "    inference_end_time = time.time()\n",
        "    inference_time = inference_end_time - inference_start_time\n",
        "\n",
        "    results[model_name] = {\n",
        "        \"checkpoint\": checkpoint,\n",
        "        \"eval_metrics\": eval_metrics,\n",
        "        \"training_time_seconds\": training_time,\n",
        "        \"inference_time_per_sample_seconds\": inference_time / len(sample_text.split())\n",
        "    }\n",
        "\n",
        "    output_model_dir_specific = f\"/content/drive/MyDrive/fine_tuned_amharic_ner_model_{model_name}\"\n",
        "    os.makedirs(output_model_dir_specific, exist_ok=True)\n",
        "\n",
        "    # Save only the best model (which the trainer already loaded at the end)\n",
        "    trainer_current.save_model(output_model_dir_specific)\n",
        "    tokenizer.save_pretrained(output_model_dir_specific)\n",
        "    print(f\"Model {model_name} saved to: {output_model_dir_specific}\")\n",
        "\n",
        "\n",
        "# Print all results in a comparison table\n",
        "print(\"\\n--- Model Comparison Results ---\")\n",
        "for model_name, data in results.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"  Checkpoint: {data['checkpoint']}\")\n",
        "    print(f\"  Evaluation Metrics (on validation set):\")\n",
        "    for metric, value in data['eval_metrics'].items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "    print(f\"  Training Time: {data['training_time_seconds']:.2f} seconds\")\n",
        "    print(f\"  Approx. Inference Time per Sample: {data['inference_time_per_sample_seconds']:.4f} seconds (for a rough sentence length)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime shap\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import shap\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Define the model path from Task 4's output\n",
        "MODEL_PATH = \"/content/drive/MyDrive/fine_tuned_amharic_ner_model_XLM-R_Amharic_NER\"\n",
        "# Define the label list used during training\n",
        "label_list = ['O', 'B-Product', 'I-Product', 'B-LOC', 'I-LOC', 'B-Money', 'I-Money']\n",
        "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
        "model.eval() # Set model to evaluation mode\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "ZFdAb9zuepkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/fine_tuned_amharic_ner_model_XLM-R_Amharic_NER\"\n",
        "# Define the label list used during training\n",
        "label_list = ['O', 'B-Product', 'I-Product', 'B-LOC', 'I-LOC', 'B-Money', 'I-Money']\n",
        "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
        "model.eval() # Set model to evaluation mode\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Example data for interpretation (from your Task 4 output)\n",
        "example_sentence = \"Electric Charcoal Burner በቀላሉ ከሰል ለማያያዝ የሚሆን አነስ ያለ ቦታ የማይዝ የሚሰራ ሻይ፣ ቡና ለማፍላት የሚሆን ዋጋ፦ 1600 ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05S06 0902660722 0928460606 በTelegram ለማዘዝ ይጠቀሙ zemencallcenter zemenexpressadmin ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን httpstelegrammezemenexpress\"\n",
        "\n",
        "def get_token_probabilities_for_target_label(text_list, target_token_idx, target_label_id):\n",
        "    \"\"\"\n",
        "    Predicts the probability of a specific label for a specific token across texts.\n",
        "    \"\"\"\n",
        "    probs_for_target_label = []\n",
        "    for text in text_list:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy() # (seq_len, num_labels)\n",
        "\n",
        "        encoded_input = tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=True)\n",
        "\n",
        "        original_tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'])\n",
        "\n",
        "        if target_token_idx < probabilities.shape[0]: # Check if index is within sequence length\n",
        "            probs_for_target_label.append(probabilities[target_token_idx, target_label_id])\n",
        "        else:\n",
        "            probs_for_target_label.append(0.0)\n",
        "    return np.array(probs_for_target_label)\n",
        "\n",
        "\n",
        "target_word = \"Electric\"\n",
        "target_label = \"B-Product\"\n",
        "target_label_id = label_to_id[target_label]\n",
        "\n",
        "encoded_example = tokenizer.encode_plus(example_sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
        "input_ids = encoded_example['input_ids'][0].tolist()\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# Find the index of 'Electric' in the *tokenized* sequence (it's usually the second token, index 1)\n",
        "target_token_in_model_output_idx = -1\n",
        "for i, token in enumerate(tokens):\n",
        "    if target_word.lower() in token.lower() and i > 0 and token != \"<s>\":\n",
        "        target_token_in_model_output_idx = i\n",
        "        break\n",
        "if target_token_in_model_output_idx == -1:\n",
        "    print(f\"Warning: Could not find '{target_word}' in the tokenized sequence.\")\n",
        "    target_token_in_model_output_idx = 1 # Fallback for demo if not found\n",
        "\n",
        "print(f\"Explaining prediction for token '{tokens[target_token_in_model_output_idx]}' (index {target_token_in_model_output_idx}) as '{target_label}'\")\n",
        "\n",
        "# Create a lambda function for KernelSHAP to explain a single output (probability of B-Product for 'Electric')\n",
        "background_texts = [example_sentence]\n",
        "\n",
        "# Create the explainer\n",
        "explainer = shap.Explainer(\n",
        "    lambda texts: get_token_probabilities_for_target_label(texts, target_token_in_model_output_idx, target_label_id),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "shap_values = explainer([example_sentence])\n",
        "\n",
        "print(f\"\\nSHAP Explanation for '{target_word}' as '{target_label}':\")\n",
        "text_data = [(token, value) for token, value in zip(shap_values[0].data, shap_values[0].values)]\n",
        "print(f\"Text token contributions to '{target_label}' prediction for '{target_word}':\")\n",
        "for token, value in text_data:\n",
        "    print(f\"  Token: '{token}', SHAP Value: {value:.4f}\")"
      ],
      "metadata": {
        "id": "zsQ2RxM_gKKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime shap\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import numpy as np\n",
        "import shap\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/fine_tuned_amharic_ner_model_XLM-R_Amharic_NER\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"Error: Model directory not found at {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"Model directory found at {MODEL_PATH}. Contents: {os.listdir(MODEL_PATH)}\")\n",
        "    # You should see files like 'config.json', 'pytorch_model.bin' etc.\n",
        "label_list = ['O', 'B-Product', 'I-Product', 'B-LOC', 'I-LOC', 'B-Money', 'I-Money']\n",
        "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
        "model.eval() # Set model to evaluation mode\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Example sentence for interpretation\n",
        "example_sentence = \"Electric Charcoal Burner በቀላሉ ከሰል ለማያያዝ የሚሆን አነስ ያለ ቦታ የማይዝ የሚሰራ ሻይ፣ ቡና ለማፍላት የሚሆን ዋጋ፦ 1600 ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05S06 0902660722 0928460606 በTelegram ለማዘዝ ይጠቀሙ zemencallcenter zemenexpressadmin ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን httpstelegrammezemenexpress\"\n",
        "\n",
        "# This part determines the specific index of \"Electric\" in the tokenized sequence.\n",
        "target_word = \"Electric\"\n",
        "encoded_example = tokenizer.encode_plus(example_sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
        "input_ids = encoded_example['input_ids'][0].tolist()\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "target_token_in_model_output_idx = -1\n",
        "for i, token in enumerate(tokens):\n",
        "    if target_word.lower() in token.lower() and i > 0 and token != \"<s>\":\n",
        "        target_token_in_model_output_idx = i\n",
        "        break\n",
        "if target_token_in_model_output_idx == -1:\n",
        "    target_token_in_model_output_idx = 1 # Fallback if not found, for demonstration\n",
        "\n",
        "# --- LIME-related functions and execution code follow here ---\n",
        "def predict_ner_token_probs(texts, original_sentence_tokens, target_original_word_idx):\n",
        "    all_target_token_probs = []\n",
        "    original_word_at_target_idx = original_sentence_tokens[target_original_word_idx]\n",
        "\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        found_subword_idx = -1\n",
        "        current_tokenized_ids = inputs['input_ids'][0].tolist()\n",
        "        current_subwords = tokenizer.convert_ids_to_tokens(current_tokenized_ids)\n",
        "\n",
        "        for i, subword in enumerate(current_subwords):\n",
        "            if original_word_at_target_idx.lower() in subword.lower() and i < probabilities.shape[0]:\n",
        "                found_subword_idx = i\n",
        "                break\n",
        "\n",
        "        if found_subword_idx != -1:\n",
        "            all_target_token_probs.append(probabilities[found_subword_idx])\n",
        "        else:\n",
        "            # If the word is not found in the perturbed text, return zeros\n",
        "            all_target_token_probs.append(np.zeros(len(label_list)))\n",
        "\n",
        "    return np.array(all_target_token_probs)\n",
        "\n",
        "original_words = example_sentence.split()\n",
        "target_word_for_lime = original_words[0]\n",
        "target_word_original_idx = 0\n",
        "\n",
        "explainer = LimeTextExplainer(\n",
        "    class_names=label_list,\n",
        "    # Use a simple whitespace split for LIME's internal segmentation.\n",
        "    # The actual BERT tokenization happens within predict_ner_token_probs.\n",
        "    split_expression=lambda text: text.split(' '),\n",
        ")\n",
        "\n",
        "print(f\"\\nLIME Explanation for '{target_word_for_lime}':\")\n",
        "\n",
        "original_sentence_split = example_sentence.split()\n",
        "lime_exp = explainer.explain_instance(\n",
        "    text_instance=example_sentence,\n",
        "    classifier_fn=lambda texts: predict_ner_token_probs(texts, original_sentence_split, target_word_original_idx),\n",
        "    labels=[label_to_id[label] for label in label_list], # All possible labels\n",
        "    num_features=5,\n",
        "    num_samples=100\n",
        ")\n",
        "\n",
        "print(f\"\\nLIME Explanation for '{original_sentence_split[target_word_original_idx]}' (original word index {target_word_original_idx}):\")\n",
        "inputs_original = tokenizer(example_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "with torch.no_grad():\n",
        "    outputs_original = model(**inputs_original)\n",
        "logits_original = outputs_original.logits\n",
        "probabilities_original = torch.softmax(logits_original, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Ensure target_token_in_model_output_idx is within bounds\n",
        "if target_token_in_model_output_idx >= probabilities_original.shape[0]:\n",
        "    print(f\"Warning: target_token_in_model_output_idx ({target_token_in_model_output_idx}) {probabilities_original.shape}. Setting to 1.\")\n",
        "    target_token_in_model_output_idx = 1 # Fallback\n",
        "\n",
        "predicted_label_id = np.argmax(probabilities_original[target_token_in_model_output_idx])\n",
        "predicted_label = id_to_label[predicted_label_id]\n",
        "print(f\"Predicted label for '{original_sentence_split[target_word_original_idx]}' is: {predicted_label}\")\n",
        "\n",
        "print(\"\\nExplanation for predicted label:\")\n",
        "for feature, weight in lime_exp.as_list(label=predicted_label_id):\n",
        "    print(f\"  - {feature}: {weight:.4f}\")\n",
        "\n",
        "print(\"\\nExplanation for all labels:\")\n",
        "for label_id in lime_exp.available_labels():\n",
        "    print(f\"\\n--- For Label: {id_to_label[label_id]} ---\")\n",
        "    for feature, weight in lime_exp.as_list(label=label_id):\n",
        "        print(f\"  - {feature}: {weight:.4f}\")"
      ],
      "metadata": {
        "id": "IhMHTaxGmzCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-running SHAP/LIME for the difficult case\n",
        "\n",
        "difficult_case_sentence = \"ይህ ምርት 500 ግራም ይመዝናል እና ዋጋው 100 ብር ነው።\"\n",
        "\n",
        "target_difficult_word = \"500\"\n",
        "target_difficult_word_original_idx = -1\n",
        "difficult_sentence_split = difficult_case_sentence.split()\n",
        "for i, word in enumerate(difficult_sentence_split):\n",
        "    if word == target_difficult_word:\n",
        "        target_difficult_word_original_idx = i\n",
        "        break\n",
        "if target_difficult_word_original_idx == -1:\n",
        "    print(f\"Warning: Could not find '{target_difficult_word}' in the difficult sentence. Setting to 0 for demo.\")\n",
        "    target_difficult_word_original_idx = 0 # Fallback for demo\n",
        "\n",
        "\n",
        "print(f\"\\n--- Analyzing Difficult Case: '{difficult_case_sentence}' ---\")\n",
        "\n",
        "# SHAP for difficult case (e.g., explaining why '500' is 'O')\n",
        "target_label_o_id = label_to_id['O']\n",
        "target_token_difficult_idx = -1\n",
        "\n",
        "encoded_difficult = tokenizer.encode_plus(difficult_case_sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
        "tokens_difficult = tokenizer.convert_ids_to_tokens(encoded_difficult['input_ids'][0].tolist())\n",
        "\n",
        "# Find the target token's index in the model's tokenized sequence (original sentence)\n",
        "found_target_in_original = False\n",
        "for i, token in enumerate(tokens_difficult):\n",
        "    if target_difficult_word.lower() in token.lower() and i > 0 and token not in tokenizer.all_special_tokens:\n",
        "        target_token_difficult_idx = i\n",
        "        found_target_in_original = True\n",
        "        break\n",
        "\n",
        "if not found_target_in_original:\n",
        "    print(f\"Warning: Could not find '{target_difficult_word}' subword in original difficult sentence tokenization. Using first content token as fallback.\")\n",
        "    for i, token in enumerate(tokens_difficult):\n",
        "        if token not in tokenizer.all_special_tokens:\n",
        "            target_token_difficult_idx = i\n",
        "            break\n",
        "    if target_token_difficult_idx == -1:\n",
        "        target_token_difficult_idx = 1 # Last resort fallback\n",
        "\n",
        "\n",
        "print(f\"Explaining prediction for token '{tokens_difficult[target_token_difficult_idx]}' (index {target_token_difficult_idx}) as 'O'\")\n",
        "\n",
        "# --- SHAP Specific Changes ---\n",
        "\n",
        "def shap_predictor(texts):\n",
        "    probabilities_for_target_label = []\n",
        "    for text_instance in texts:\n",
        "\n",
        "        if isinstance(text_instance, np.ndarray):\n",
        "            text_str = text_instance[0] if text_instance.size > 0 else \"\"\n",
        "        elif isinstance(text_instance, str):\n",
        "            text_str = text_instance\n",
        "        else:\n",
        "\n",
        "            probabilities_for_target_label.append(0.0)\n",
        "            continue\n",
        "\n",
        "        if not text_str:\n",
        "            probabilities_for_target_label.append(0.0)\n",
        "            continue\n",
        "\n",
        "        inputs = tokenizer(text_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        current_tokens_ids = inputs['input_ids'][0].tolist()\n",
        "        current_tokens = tokenizer.convert_ids_to_tokens(current_tokens_ids)\n",
        "\n",
        "        current_target_idx_in_perturbed = -1\n",
        "\n",
        "        for i, tok in enumerate(current_tokens):\n",
        "            if target_difficult_word.lower() in tok.lower() and i < probabilities.shape[0] and tok not in tokenizer.all_special_tokens:\n",
        "                current_target_idx_in_perturbed = i\n",
        "                break\n",
        "\n",
        "        if current_target_idx_in_perturbed == -1:\n",
        "            probabilities_for_target_label.append(0.0)\n",
        "            continue\n",
        "\n",
        "        prob_target_label = probabilities[current_target_idx_in_perturbed, target_label_o_id]\n",
        "        probabilities_for_target_label.append(prob_target_label)\n",
        "\n",
        "    return np.array(probabilities_for_target_label).reshape(-1, 1)\n",
        "\n",
        "explainer_difficult = shap.Explainer(\n",
        "    model=shap_predictor,\n",
        "    masker=shap.maskers.Text(mask_token=tokenizer.mask_token, tokenizer=tokenizer)\n",
        ")\n",
        "\n",
        "shap_values_difficult = explainer_difficult([difficult_case_sentence])\n",
        "\n",
        "print(f\"\\nSHAP Explanation for '{target_difficult_word}' as 'O' in difficult case:\")\n",
        "if len(shap_values_difficult) > 0:\n",
        "    for i, instance in enumerate(shap_values_difficult):\n",
        "        if hasattr(instance, 'data') and hasattr(instance, 'values'):\n",
        "            text_data_difficult = [(token, value) for token, value in zip(instance.data, instance.values)]\n",
        "            print(f\"Explanation for instance {i}:\")\n",
        "            for token, value in text_data_difficult:\n",
        "                # FIX: Access the scalar value from the numpy array\n",
        "                if isinstance(value, np.ndarray) and value.size == 1:\n",
        "                    actual_value = value.item()y\n",
        "                else:\n",
        "                    actual_value = value # Assume it's already a scalar if not a 1-element array\n",
        "                print(f\"  Token: '{token}', SHAP Value: {actual_value:.4f}\")\n",
        "        else:\n",
        "            print(f\"Could not retrieve data and values for SHAP instance {i}. Type: {type(instance)}\")\n",
        "else:\n",
        "    print(\"No SHAP values generated.\")\n",
        "\n",
        "\n",
        "# LIME for difficult case (e.g., explaining why '500' is 'O')\n",
        "\n",
        "explainer = LimeTextExplainer(\n",
        "    class_names=label_list,\n",
        "    split_expression=lambda text: text.split(' '),\n",
        ")\n",
        "\n",
        "def predict_ner_token_probs(texts, original_sentence_tokens, target_original_word_idx):\n",
        "    all_target_token_probs = []\n",
        "    original_word_at_target_idx = original_sentence_tokens[target_original_word_idx]\n",
        "\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        found_subword_idx = -1\n",
        "        current_tokenized_ids = inputs['input_ids'][0].tolist()\n",
        "        current_subwords = tokenizer.convert_ids_to_tokens(current_tokenized_ids)\n",
        "\n",
        "        for i, subword in enumerate(current_subwords):\n",
        "\n",
        "            if original_word_at_target_idx.lower() in subword.lower() and i < probabilities.shape[0]:\n",
        "                found_subword_idx = i\n",
        "                break\n",
        "\n",
        "        if found_subword_idx != -1:\n",
        "            all_target_token_probs.append(probabilities[found_subword_idx])\n",
        "        else:\n",
        "            all_target_token_probs.append(np.zeros(len(label_list)))\n",
        "\n",
        "    return np.array(all_target_token_probs)\n",
        "\n",
        "\n",
        "lime_exp_difficult = explainer.explain_instance(\n",
        "    text_instance=difficult_case_sentence,\n",
        "    classifier_fn=lambda texts: predict_ner_token_probs(texts, difficult_sentence_split, target_difficult_word_original_idx),\n",
        "    labels=[label_to_id[label] for label in label_list],\n",
        "    num_features=5,\n",
        "    num_samples=100\n",
        ")\n",
        "\n",
        "inputs_difficult = tokenizer(difficult_case_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "with torch.no_grad():\n",
        "    outputs_difficult = model(**inputs_difficult)\n",
        "logits_difficult = outputs_difficult.logits\n",
        "probabilities_difficult = torch.softmax(logits_difficult, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "if target_token_difficult_idx >= probabilities_difficult.shape[0]:\n",
        "    print(f\"Warning: target_token_difficult_idx ({target_token_difficult_idx}) is out of bounds for model output shape {probabilities_difficult.shape}. Adjusting to nearest valid index.\")\n",
        "    if probabilities_difficult.shape[0] > 1:\n",
        "        target_token_difficult_idx = 1\n",
        "    else:\n",
        "        target_token_difficult_idx = 0\n",
        "\n",
        "\n",
        "predicted_label_id_difficult = np.argmax(probabilities_difficult[target_token_difficult_idx])\n",
        "predicted_label_name_difficult = id_to_label[predicted_label_id_difficult]\n",
        "\n",
        "print(f\"\\nModel's Predicted Label for '{tokens_difficult[target_token_difficult_idx]}' in difficult case is: {predicted_label_name_difficult} (Prob: {probabilities_difficult[target_token_difficult_idx, predicted_label_id_difficult]:.4f})\")\n",
        "print(f\"Contribution of words to predicting '{predicted_label_name_difficult}' for '{difficult_sentence_split[target_difficult_word_original_idx]}' (LIME):\")\n",
        "\n",
        "for feature, weight in lime_exp_difficult.as_list(label=predicted_label_id_difficult):\n",
        "    print(f\"  Word: '{feature}', Contribution: {weight:.4f}\")"
      ],
      "metadata": {
        "id": "qvdD9FyivFgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TASK 6: FINTECH VENDOR SCORECARD FOR MICRO-LENDING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "synthetic_posts_data = [\n",
        "    {\n",
        "        \"vendor_id\": \"EthioTechMart\",\n",
        "        \"post_text\": \"Electric Charcoal Burner በቀላሉ ከሰል ለማያያዝ የሚሆን አነስ ያለ ቦታ የማይዝ የሚሰራ ሻይ፣ ቡና ለማፍላት የሚሆን ዋጋ፦ 1600 ብር ውስን ፍሬ ነው ያለው አድራሻ መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05S06 0902660722 0928460606 በTelegram ለማዘዝ ይጠቀሙ zemencallcenter zemenexpressadmin ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን httpstelegrammezemenexpress\",\n",
        "        \"timestamp\": \"2025-06-20 10:00:00\",\n",
        "        \"views\": 5200,\n",
        "        \"ner_labels\": [('Electric Charcoal Burner', 'Product'), ('1600', 'Money'), ('መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'LOC')]\n",
        "    },\n",
        "    {\n",
        "        \"vendor_id\": \"EthioTechMart\",\n",
        "        \"post_text\": \"አዲስ አይፎን 15 ፕሮ ማክስ በ200,000 ብር ብቻ። ፈጥነው ይዘዙ!\",\n",
        "        \"timestamp\": \"2025-06-22 14:30:00\",\n",
        "        \"views\": 8500,\n",
        "        \"ner_labels\": [('አይፎን 15 ፕሮ ማክስ', 'Product'), ('200,000', 'Money')]\n",
        "    },\n",
        "    {\n",
        "        \"vendor_id\": \"EthioTechMart\",\n",
        "        \"post_text\": \"ምርጥ የቡና መፍጫ ማሽን 3500 ብር። አድራሻ ቦሌ ሩዋንዳ.\",\n",
        "        \"timestamp\": \"2025-06-25 09:15:00\",\n",
        "        \"views\": 4800,\n",
        "        \"ner_labels\": [('የቡና መፍጫ ማሽን', 'Product'), ('3500', 'Money'), ('ቦሌ ሩዋንዳ', 'LOC')]\n",
        "    },\n",
        "    {\n",
        "        \"vendor_id\": \"EthioTechMart\",\n",
        "        \"post_text\": \"የቤት እቃዎች ቅናሽ! ለበለጠ መረጃ ይደውሉልን።\",\n",
        "        \"timestamp\": \"2025-06-28 11:00:00\",\n",
        "        \"views\": 3100,\n",
        "        \"ner_labels\": [] # No specific product/money entity extracted\n",
        "    },\n",
        "    {\n",
        "        \"vendor_id\": \"FashionEthio\",\n",
        "        \"post_text\": \"የሴቶች ፋሽን ልብሶች። ዋጋ 800 ብር። አድራሻ ሜክሲኮ.\",\n",
        "        \"timestamp\": \"2025-06-18 16:00:00\",\n",
        "        \"views\": 2500,\n",
        "        \"ner_labels\": [('የሴቶች ፋሽን ልብሶች', 'Product'), ('800', 'Money'), ('ሜክሲኮ', 'LOC')]\n",
        "    },\n",
        "    {\n",
        "        \"vendor_id\": \"FashionEthio\",\n",
        "        \"post_text\": \"ቄንጠኛ ጫማዎች በ1200 ብር።\",\n",
        "        \"timestamp\": \"2025-06-20 10:30:00\",\n",
        "        \"views\": 3200,\n",
        "        \"ner_labels\": [('ጫማዎች', 'Product'), ('1200', 'Money')]\n",
        "    },\n",
        "    {\n",
        "        \"vendor_id\": \"FashionEthio\",\n",
        "        \"post_text\": \"ልዩ ልዩ ሽመናዎች። በታላቅ ቅናሽ! ይምጡና ይጎብኙን።\",\n",
        "        \"timestamp\": \"2025-06-21 18:00:00\",\n",
        "        \"views\": 1800,\n",
        "        \"ner_labels\": [('ሽመናዎች', 'Product')]\n",
        "    },\n",
        "    {\n",
        "        \"vendor_id\": \"FashionEthio\",\n",
        "        \"post_text\": \"የባህል ልብሶች 1500 ብር። አድራሻ ፒያሳ.\",\n",
        "        \"timestamp\": \"2025-06-26 09:00:00\",\n",
        "        \"views\": 2900,\n",
        "        \"ner_labels\": [('የባህል ልብሶች', 'Product'), ('1500', 'Money'), ('ፒያሳ', 'LOC')]\n",
        "    },\n",
        "    {\n",
        "        \"vendor_id\": \"EthioCars\",\n",
        "        \"post_text\": \"የ2020 ቶዮታ ካምሪ ለሽያጭ ቀርቧል። ዋጋ፦ 3 ሚሊየን ብር።\",\n",
        "        \"timestamp\": \"2025-06-19 12:00:00\",\n",
        "        \"views\": 15000,\n",
        "        \"ner_labels\": [('ቶዮታ ካምሪ', 'Product'), ('3 ሚሊየን', 'Money')]\n",
        "    },\n",
        "    {\n",
        "        \"vendor_id\": \"EthioCars\",\n",
        "        \"post_text\": \"አዳዲስ የመርሴዲስ መኪኖች አሉ።\",\n",
        "        \"timestamp\": \"2025-06-27 10:00:00\",\n",
        "        \"views\": 9000,\n",
        "        \"ner_labels\": [('መርሴዲስ መኪኖች', 'Product')]\n",
        "    }\n",
        "]\n",
        "\n",
        "df_posts = pd.DataFrame(synthetic_posts_data)\n",
        "df_posts['timestamp'] = pd.to_datetime(df_posts['timestamp'])\n",
        "\n",
        "\n",
        "def get_ner_predictions(text, model, tokenizer, id_to_label):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
        "    entities = []\n",
        "    current_entity = []\n",
        "    current_entity_type = None\n",
        "\n",
        "    for i, (token_id, pred_id) in enumerate(zip(inputs[\"input_ids\"].squeeze().tolist(), predictions)):\n",
        "        token = tokenizer.decode([token_id])\n",
        "\n",
        "        if token in ['<s>', '</s>', '<pad>']:\n",
        "            if current_entity: # If there's an active entity before a special token, close it\n",
        "                entities.append((\" \".join(current_entity).replace(\" ##\", \"\"), current_entity_type))\n",
        "            current_entity = []\n",
        "            current_entity_type = None\n",
        "            continue\n",
        "\n",
        "        pred_label = id_to_label[pred_id]\n",
        "\n",
        "        if pred_label.startswith(\"B-\"):\n",
        "            if current_entity: # Close previous entity if exists\n",
        "                entities.append((\" \".join(current_entity).replace(\" ##\", \"\"), current_entity_type))\n",
        "            current_entity = [token.replace(\" \", \"\")]\n",
        "            current_entity_type = pred_label[2:]\n",
        "        elif pred_label.startswith(\"I-\"):\n",
        "            if current_entity and pred_label[2:] == current_entity_type:\n",
        "                current_entity.append(token.replace(\" \", \"\"))\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    entities.append((\" \".join(current_entity).replace(\" ##\", \"\"), current_entity_type))\n",
        "                current_entity = [token.replace(\" \", \"\")]\n",
        "                current_entity_type = pred_label[2:] # Start new entity\n",
        "        else:\n",
        "            if current_entity:\n",
        "                entities.append((\" \".join(current_entity).replace(\" ##\", \"\"), current_entity_type))\n",
        "            current_entity = []\n",
        "            current_entity_type = None\n",
        "    if current_entity:\n",
        "        entities.append((\" \".join(current_entity).replace(\" ##\", \"\"), current_entity_type))\n",
        "    return entities\n",
        "\n",
        "# --- Develop a Vendor Analytics Engine ---\n",
        "class VendorAnalyticsEngine:\n",
        "    def __init__(self, posts_df, ner_model, ner_tokenizer, id_to_label_map):\n",
        "        self.posts_df = posts_df\n",
        "        self.ner_model = ner_model\n",
        "        self.ner_tokenizer = ner_tokenizer\n",
        "        self.id_to_label_map = id_to_label_map\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.ner_model.to(self.device)\n",
        "        self.ner_model.eval()\n",
        "\n",
        "    def _get_ner_entities_from_text(self, text):\n",
        "        \"\"\"Helper to get NER entities for a given text.\"\"\"\n",
        "        return get_ner_predictions(text, self.ner_model, self.ner_tokenizer, self.id_to_label_map)\n",
        "\n",
        "    def calculate_vendor_metrics(self, vendor_id):\n",
        "        vendor_posts = self.posts_df[self.posts_df['vendor_id'] == vendor_id].copy()\n",
        "\n",
        "        if vendor_posts.empty:\n",
        "            return None\n",
        "\n",
        "        # Sort posts by timestamp for consistency calculations\n",
        "        vendor_posts = vendor_posts.sort_values(by='timestamp')\n",
        "\n",
        "        # 1. Activity & Consistency: Posting Frequency (Posts per Week)\n",
        "        if len(vendor_posts) > 1:\n",
        "            time_span_days = (vendor_posts['timestamp'].max() - vendor_posts['timestamp'].min()).days\n",
        "            if time_span_days == 0:\n",
        "                posting_frequency = len(vendor_posts) * 7\n",
        "            else:\n",
        "                posting_frequency = (len(vendor_posts) / time_span_days) * 7\n",
        "        else:\n",
        "            posting_frequency = 1.0\n",
        "\n",
        "        average_views_per_post = vendor_posts['views'].mean()\n",
        "\n",
        "        top_post = vendor_posts.loc[vendor_posts['views'].idxmax()]\n",
        "        top_product_ner = [ent for ent in top_post['ner_labels'] if ent[1] == 'Product']\n",
        "\n",
        "        top_price_ner = [ent for ent in top_post['ner_labels'] if ent[1] == 'Money']\n",
        "\n",
        "        top_performing_product = top_product_ner[0][0] if top_product_ner else \"N/A\"\n",
        "        top_performing_price = self._parse_price(top_price_ner[0][0]) if top_price_ner else None\n",
        "\n",
        "        all_prices = []\n",
        "        for _, row in vendor_posts.iterrows():\n",
        "            for entity, entity_type in row['ner_labels']:\n",
        "                if entity_type == 'Money':\n",
        "                    price = self._parse_price(entity)\n",
        "                    if price is not None:\n",
        "                        all_prices.append(price)\n",
        "\n",
        "        average_price_point = np.mean(all_prices) if all_prices else 0.0\n",
        "\n",
        "        return {\n",
        "            \"vendor_id\": vendor_id,\n",
        "            \"posting_frequency\": posting_frequency,\n",
        "            \"average_views_per_post\": average_views_per_post,\n",
        "            \"top_performing_post_views\": top_post['views'],\n",
        "            \"top_performing_post_product\": top_performing_product,\n",
        "            \"top_performing_post_price\": top_performing_price,\n",
        "            \"average_price_point\": average_price_point\n",
        "        }\n",
        "\n",
        "    def _parse_price(self, price_string):\n",
        "        price_string = price_string.lower().replace(\",\", \"\")\n",
        "        price = None\n",
        "        try:\n",
        "            if \"ሚሊየን\" in price_string or \"million\" in price_string:\n",
        "\n",
        "                num_part = re.search(r'(\\d+\\.?\\d*)\\s*(ሚሊየን|million)', price_string)\n",
        "                if num_part:\n",
        "                    price = float(num_part.group(1)) * 1_000_000\n",
        "            elif \"ሺህ\" in price_string or \"k\" in price_string:\n",
        "                num_part = re.search(r'(\\d+\\.?\\d*)\\s*(ሺህ|k)', price_string)\n",
        "                if num_part:\n",
        "                    price = float(num_part.group(1)) * 1_000\n",
        "            else:\n",
        "                price = float(re.sub(r'[^\\d.]', '', price_string))\n",
        "        except ValueError:\n",
        "            pass # Return None if parsing fails\n",
        "        return price\n",
        "\n",
        "    def calculate_all_vendors_scores(self):\n",
        "        vendor_ids = self.posts_df['vendor_id'].unique()\n",
        "        all_vendor_metrics = []\n",
        "        for vendor_id in vendor_ids:\n",
        "            metrics = self.calculate_vendor_metrics(vendor_id)\n",
        "            if metrics:\n",
        "                all_vendor_metrics.append(metrics)\n",
        "        return all_vendor_metrics\n",
        "\n",
        "    def assign_lending_score(self, vendor_metrics):\n",
        "\n",
        "        max_views = self.posts_df['views'].max() if not self.posts_df.empty else 1\n",
        "\n",
        "        all_frequencies = []\n",
        "        for vendor_id in self.posts_df['vendor_id'].unique():\n",
        "            vendor_posts_temp = self.posts_df[self.posts_df['vendor_id'] == vendor_id].copy()\n",
        "            if len(vendor_posts_temp) > 1:\n",
        "                time_span_days_temp = (vendor_posts_temp['timestamp'].max() - vendor_posts_temp['timestamp'].min()).days\n",
        "                if time_span_days_temp == 0:\n",
        "                    all_frequencies.append(len(vendor_posts_temp) * 7)\n",
        "                else:\n",
        "                    all_frequencies.append((len(vendor_posts_temp) / time_span_days_temp) * 7)\n",
        "            elif len(vendor_posts_temp) == 1: # Only one post\n",
        "                all_frequencies.append(1.0) # Assume 1 post per week for a single post\n",
        "            else: # No posts for this vendor\n",
        "                all_frequencies.append(0.0) # Or some other neutral value\n",
        "\n",
        "        max_frequency = max(all_frequencies) if all_frequencies else 1.0\n",
        "        if max_frequency == 0:\n",
        "            max_frequency = 1.0\n",
        "\n",
        "        all_extracted_prices = []\n",
        "        for _, row in self.posts_df.iterrows():\n",
        "            for entity, entity_type in row['ner_labels']:\n",
        "                if entity_type == 'Money':\n",
        "                    price = self._parse_price(entity)\n",
        "                    if price is not None:\n",
        "                        all_extracted_prices.append(price)\n",
        "        max_avg_price = np.mean(all_extracted_prices) * 2 if all_extracted_prices else 100000 # A heuristic max\n",
        "\n",
        "        norm_views = vendor_metrics['average_views_per_post'] / max_views\n",
        "        norm_freq = vendor_metrics['posting_frequency'] / max_frequency\n",
        "        norm_price = vendor_metrics['average_price_point'] / max_avg_price if max_avg_price > 0 else 0\n",
        "\n",
        "        # Define weights (can be adjusted based on business logic)\n",
        "        weight_views = 0.4\n",
        "        weight_frequency = 0.3\n",
        "        weight_price = 0.3\n",
        "\n",
        "        lending_score = (norm_views * weight_views) + \\\n",
        "                        (norm_freq * weight_frequency) + \\\n",
        "                        (norm_price * weight_price)\n",
        "\n",
        "        # Scale to a more intuitive range, e.g., 0-100\n",
        "        lending_score_scaled = lending_score * 100\n",
        "        return round(lending_score_scaled, 2)\n",
        "\n",
        "# --- Main execution for Task 6 ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        model\n",
        "        tokenizer\n",
        "        id_to_label\n",
        "        device\n",
        "    except NameError:\n",
        "        print(\"Warning: `model`, `tokenizer`, `id_to_label`, `device` are not defined globally. Please ensure they are loaded in previous cells or define placeholders here.\")\n",
        "        # Minimal placeholders to allow the code to run for demonstration purposes\n",
        "        class MockTokenizer:\n",
        "            def __call__(self, text, return_tensors, padding, truncation, max_length):\n",
        "                return {'input_ids': torch.tensor([[0, 1, 2, 3, 4]]), 'attention_mask': torch.tensor([[1, 1, 1, 1, 1]])}\n",
        "            def decode(self, ids):\n",
        "                return f\"tok_{ids[0]}\"\n",
        "            def convert_ids_to_tokens(self, ids):\n",
        "                return [f\"tok_{i}\" for i in ids]\n",
        "        class MockModel(torch.nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.logits = torch.randn(1, 5, 7)\n",
        "            def __call__(self, **kwargs):\n",
        "                return type('obj', (object,), {'logits': self.logits})()\n",
        "            def to(self, device): pass\n",
        "            def eval(self): pass\n",
        "\n",
        "        tokenizer = MockTokenizer()\n",
        "        model = MockModel()\n",
        "        device = torch.device(\"cpu\")\n",
        "        label_list = [\"O\", \"B-Product\", \"I-Product\", \"B-Money\", \"I-Money\", \"B-LOC\", \"I-LOC\"]\n",
        "        id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "        label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "\n",
        "    # Create the analytics engine\n",
        "    analytics_engine = VendorAnalyticsEngine(df_posts, model, tokenizer, id_to_label)\n",
        "\n",
        "    # Calculate metrics for all vendors\n",
        "    all_vendor_metrics_data = analytics_engine.calculate_all_vendors_scores()\n",
        "\n",
        "    # Create the final scorecard table\n",
        "    scorecard_data = []\n",
        "    for vendor_data in all_vendor_metrics_data:\n",
        "        lending_score = analytics_engine.assign_lending_score(vendor_data)\n",
        "        scorecard_data.append({\n",
        "            \"Vendor ID\": vendor_data['vendor_id'],\n",
        "            \"Avg. Views/Post\": round(vendor_data['average_views_per_post']),\n",
        "            \"Posts/Week\": round(vendor_data['posting_frequency'], 2),\n",
        "            \"Avg. Price (ETB)\": round(vendor_data['average_price_point'], 2),\n",
        "            \"Top Post Product\": vendor_data['top_performing_post_product'],\n",
        "            \"Top Post Price\": vendor_data['top_performing_post_price'],\n",
        "            \"Lending Score\": lending_score\n",
        "        })\n",
        "\n",
        "    # Sort by Lending Score for better presentation\n",
        "    vendor_scorecard_df = pd.DataFrame(scorecard_data).sort_values(by=\"Lending Score\", ascending=False)\n",
        "\n",
        "    print(\"\\n--- Vendor Scorecard ---\")\n",
        "    print(vendor_scorecard_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n--- Vendor Scorecard Analysis ---\")\n",
        "    print(\"The 'Lending Score' is a composite metric designed to identify promising vendors for micro-lending.\")\n",
        "    print(\"It combines aspects of activity, market reach, and business profile.\")\n",
        "    print(\"  - **Avg. Views/Post**: Indicates customer interest and reach. Higher is better.\")\n",
        "    print(\"  - **Posts/Week**: Shows consistency and activity level. Higher is better.\")\n",
        "    print(\"  - **Avg. Price (ETB)**: Gives insight into the typical price point of products. This could influence loan size and risk assessment.\")\n",
        "    print(\"  - **Top Post Product/Price**: Highlights successful products and their associated price.\")\n",
        "    print(\"  - **Lending Score**: Our custom score, normalized 0-100. Higher scores suggest more active and engaging vendors.\")\n",
        "    print(\"\\nBased on this synthetic data:\")\n",
        "    print(f\"- **{vendor_scorecard_df.iloc[0]['Vendor ID']}** appears to be the most promising due to its high views and consistent posting.\")\n",
        "    print(\"  - Businesses with higher average views per post and more frequent posting generally indicate stronger engagement and a more active business, making them potentially lower risk for micro-lending.\")\n",
        "    print(\"  - The average price point helps understand the scale of products they deal with, which can inform loan amounts.\")"
      ],
      "metadata": {
        "id": "D_NhhyJNznm1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}